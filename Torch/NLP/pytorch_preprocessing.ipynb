{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "parahagraph = \"\"\"Abhishek Waghchaure\n",
    "Sai Vrundavan, flat no-703, Sr No-56/5B,\n",
    "Behind Abhiruchi Mall,\n",
    "Sinhgad road, Vadgaon Bk.,\n",
    "PUNE, Maharashtra - 411041\n",
    "Email: abhisw28@gmail.com\n",
    "Mobile: 8668566528\n",
    "Dear Human Resources Team\n",
    "I am excited to apply for the Data Science Engineer position. With a strong academic foundation in\n",
    "data science and analytics, complemented by practical experience as a software developer, I am\n",
    "eager to bring my skills to your team and contribute to the innovative AI-driven solutions your\n",
    "organization is known for.\n",
    "My M.Tech in Data Science and Analytics has provided me with deep insights into machine\n",
    "learning, data analysis, and predictive modeling. Although my professional experience at Vinz\n",
    "Global and Aventior Digital Pvt Ltd primarily involved software development, these roles honed my\n",
    "technical expertise, particularly in backend development, API improvement, and database\n",
    "migration. These experiences have given me a solid understanding of the full software development\n",
    "lifecycle, which is crucial for building and optimizing data pipelines, implementing machine\n",
    "learning models, and deploying data-driven applications.\n",
    "One of my significant academic projects involved developing a skin cancer detection and\n",
    "classification model using convolutional neural networks (CNNs). This project not only enhanced\n",
    "my machine learning skills but also highlighted the importance of accuracy and innovation in\n",
    "developing life-impacting solutions. Additionally, my work on a diamond price prediction model\n",
    "using linear regression demonstrated my ability to analyze complex datasets and create precise,\n",
    "data-driven forecasts.\n",
    "I am particularly enthusiastic about this role because it offers the opportunity to transition my strong\n",
    "technical foundation and passion for data science into a position focused on developing and\n",
    "optimizing AI solutions. I am eager to collaborate with your multi-functional team, leverage my\n",
    "problem-solving skills, and contribute to driving the success of your AI-assisted decision-making\n",
    "processes.\n",
    "Thank you for considering my application. I am looking forward to the opportunity to discuss how\n",
    "my background in software development, combined with my academic focus on data science, can\n",
    "contribute to the innovative projects. Please feel free to contact me at 8668566528 or via email at\n",
    "abhisw28@gmail.com to schedule an interview.\n",
    "Sincerely,\n",
    "Abhishek Waghchaure\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset,TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    corpus = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Remove non-alphabetic characters\n",
    "        review = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "        review = review.lower()\n",
    "        \n",
    "        # Tokenize the cleaned sentence\n",
    "        words = nltk.word_tokenize(review)\n",
    "        \n",
    "        # Stemming and removing stopwords\n",
    "        stemmed_words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "        \n",
    "        # Join the processed words back into a sentence and add to the corpus\n",
    "        processed_sentence = ' '.join(stemmed_words)\n",
    "        corpus.append(processed_sentence)\n",
    "    \n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_senteces(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    encoded_data = vectorizer.fit_transform(sentences)\n",
    "    return encoded_data.toarray(), vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbhishekCoverLetter(Dataset):\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing_pipeline(sentences):\n",
    "    preprocessed_sentences = preprocess_text(sentences)\n",
    "    encoded_sentences, vectorizer = encode_senteces(preprocessed_sentences)\n",
    "    dataset = AbhishekCoverLetter(encoded_sentences)\n",
    "    train_loader = DataLoader(dataset=dataset, shuffle=True, batch_size=2 )\n",
    "    return train_loader, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, vectorizer = text_processing_pipeline(parahagraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abhiruchi' 'abhishek' 'abhisw' 'abil' 'academ' 'accuraci' 'addit' 'ai'\n",
      " 'also' 'although' 'analysi' 'analyt' 'analyz' 'api' 'appli' 'applic'\n",
      " 'assist' 'aventior' 'backend' 'background' 'behind' 'bk' 'bring' 'build'\n",
      " 'cancer' 'classif' 'cnn' 'collabor' 'com' 'combin' 'complement' 'complex'\n",
      " 'consid' 'contact' 'contribut' 'convolut' 'creat' 'crucial' 'data'\n",
      " 'databas' 'dataset' 'dear' 'decis' 'deep' 'demonstr' 'deploy' 'detect'\n",
      " 'develop' 'diamond' 'digit' 'discuss' 'drive' 'driven' 'eager' 'email'\n",
      " 'engin' 'enhanc' 'enthusiast' 'excit' 'experi' 'expertis' 'feel' 'flat'\n",
      " 'focu' 'focus' 'forecast' 'forward' 'foundat' 'free' 'full' 'function'\n",
      " 'given' 'global' 'gmail' 'highlight' 'hone' 'human' 'impact' 'implement'\n",
      " 'import' 'improv' 'innov' 'insight' 'interview' 'involv' 'known' 'learn'\n",
      " 'leverag' 'life' 'lifecycl' 'linear' 'look' 'ltd' 'machin' 'maharashtra'\n",
      " 'make' 'mall' 'migrat' 'mobil' 'model' 'multi' 'network' 'neural' 'offer'\n",
      " 'one' 'opportun' 'optim' 'organ' 'particularli' 'passion' 'pipelin'\n",
      " 'pleas' 'posit' 'practic' 'precis' 'predict' 'price' 'primarili'\n",
      " 'problem' 'process' 'profession' 'project' 'provid' 'pune' 'pvt'\n",
      " 'regress' 'resourc' 'road' 'role' 'sai' 'schedul' 'scienc' 'signific'\n",
      " 'sincer' 'sinhgad' 'skill' 'skin' 'softwar' 'solid' 'solut' 'solv' 'sr'\n",
      " 'strong' 'success' 'team' 'tech' 'technic' 'thank' 'transit' 'understand'\n",
      " 'use' 'vadgaon' 'via' 'vinz' 'vrundavan' 'waghchaur' 'work']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
