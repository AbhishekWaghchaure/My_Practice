{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b26e158e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/opt/anaconda3/envs/stable_env_nlp/lib/python3.10/site-packages/torchtext/_torchtext.so, 0x0002): Symbol not found: __ZN2at4_ops10select_int4callERKNS_6TensorExx\n  Referenced from: <DC3EC60B-CD3A-351D-B3E8-F0164AD364E9> /opt/anaconda3/envs/stable_env_nlp/lib/python3.10/site-packages/torchtext/_torchtext.so\n  Expected in:     <ECC148AF-20FF-3EEE-BC75-4DD3E7455393> /opt/anaconda3/envs/stable_env_nlp/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Multi30k\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_vocab_from_iterator\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/stable_env_nlp/lib/python3.10/site-packages/torchtext/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vocab\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional\n",
      "File \u001b[0;32m/opt/anaconda3/envs/stable_env_nlp/lib/python3.10/site-packages/torchtext/vocab/__init__.py:11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     GloVe,\n\u001b[1;32m      5\u001b[0m     FastText,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     Vectors,\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     vocab,\n\u001b[1;32m     13\u001b[0m     build_vocab_from_iterator,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocab\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_vocab_from_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained_aliases\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectors\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/stable_env_nlp/lib/python3.10/site-packages/torchtext/vocab/vocab_factory.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Iterable, Optional, List\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter, OrderedDict\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_torchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     Vocab \u001b[38;5;28;01mas\u001b[39;00m VocabPybind,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvocab\u001b[39m(ordered_dict: Dict, min_freq: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     10\u001b[0m           specials: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m           special_first: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Vocab:\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Factory method for creating a vocab object which maps tokens to indices.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    Note that the ordering in which key value pairs were inserted in the `ordered_dict` will be respected when building the vocab.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m        >>> v2['out of vocab'] is v2[unk_token] #prints True\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/opt/anaconda3/envs/stable_env_nlp/lib/python3.10/site-packages/torchtext/_torchtext.so, 0x0002): Symbol not found: __ZN2at4_ops10select_int4callERKNS_6TensorExx\n  Referenced from: <DC3EC60B-CD3A-351D-B3E8-F0164AD364E9> /opt/anaconda3/envs/stable_env_nlp/lib/python3.10/site-packages/torchtext/_torchtext.so\n  Expected in:     <ECC148AF-20FF-3EEE-BC75-4DD3E7455393> /opt/anaconda3/envs/stable_env_nlp/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ae4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English and German tokenizers\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_de = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26e644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello INTJ\n",
      ", PUNCT\n",
      "how SCONJ\n",
      "are AUX\n",
      "you PRON\n",
      "? PUNCT\n",
      "Hallo PROPN\n",
      ", PUNCT\n",
      "wie ADV\n",
      "geht VERB\n",
      "es PRON\n",
      "dir PRON\n",
      "? PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Process English text\n",
    "doc_eng = spacy_en(\"Hello, how are you?\")\n",
    "for token in doc_eng:\n",
    "    print(token.text, token.pos_)  # Print each token and its part-of-speech tag\n",
    "\n",
    "# Process German text\n",
    "doc_ger = spacy_de(\"Hallo, wie geht es dir?\")\n",
    "for token in doc_ger:\n",
    "    print(token.text, token.pos_)  # Print each token and its part-of-speech tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a661a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"I am excited to apply for the Data Science Engineer position. With a strong academic foundation in\n",
    "data science and analytics, complemented by practical experience as a software developer, I am\n",
    "eager to bring my skills to your team and contribute to the innovative AI-driven solutions your\n",
    "organization is known for.\n",
    "My M.Tech in Data Science and Analytics has provided me with deep insights into machine\n",
    "learning, data analysis, and predictive modeling. Although my professional experience at Vinz\n",
    "Global and Aventior Digital Pvt Ltd primarily involved software development, these roles honed my\n",
    "technical expertise, particularly in backend development, API improvement, and database\n",
    "migration. These experiences have given me a solid understanding of the full software development\n",
    "lifecycle, which is crucial for building and optimizing data pipelines, implementing machine\n",
    "learning models, and deploying data-driven applications.\n",
    "One of my significant academic projects involved developing a skin cancer detection and\n",
    "classification model using convolutional neural networks (CNNs). This project not only enhanced\n",
    "my machine learning skills but also highlighted the importance of accuracy and innovation in\n",
    "developing life-impacting solutions. Additionally, my work on a diamond price prediction model\n",
    "using linear regression demonstrated my ability to analyze complex datasets and create precise,\n",
    "data-driven forecasts.\n",
    "I am particularly enthusiastic about this role because it offers the opportunity to transition my strong\n",
    "technical foundation and passion for data science into a position focused on developing and\n",
    "optimizing AI solutions. I am eager to collaborate with your multi-functional team, leverage my\n",
    "problem-solving skills, and contribute to driving the success of your AI-assisted decision-making\n",
    "processes.\n",
    "Thank you for considering my application. I am looking forward to the opportunity to discuss how\n",
    "my background in software development, combined with my academic focus on data science, can\n",
    "contribute to the innovative projects. Please feel free to contact me at 8668566528 or via email at\n",
    "abhisw28@gmail.com to schedule an interview.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f7ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = spacy_en(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec9c942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am excited to apply for the Data Science Engineer position.\n",
      "With a strong academic foundation in\n",
      "data science and analytics, complemented by practical experience as a software developer, I am\n",
      "eager to bring my skills to your team and contribute to the innovative AI-driven solutions your\n",
      "organization is known for.\n",
      "\n",
      "My M.Tech in Data Science and Analytics has provided me with deep insights into machine\n",
      "learning, data analysis, and predictive modeling.\n",
      "Although my professional experience at Vinz\n",
      "Global and Aventior Digital Pvt Ltd primarily involved software development, these roles honed my\n",
      "technical expertise, particularly in backend development, API improvement, and database\n",
      "migration.\n",
      "These experiences have given me a solid understanding of the full software development\n",
      "lifecycle, which is crucial for building and optimizing data pipelines, implementing machine\n",
      "learning models, and deploying data-driven applications.\n",
      "\n",
      "One of my significant academic projects involved developing a skin cancer detection and\n",
      "classification model using convolutional neural networks (CNNs).\n",
      "This project not only enhanced\n",
      "my machine learning skills but also highlighted the importance of accuracy and innovation in\n",
      "developing life-impacting solutions.\n",
      "Additionally, my work on a diamond price prediction model\n",
      "using linear regression demonstrated my ability to analyze complex datasets and create precise,\n",
      "data-driven forecasts.\n",
      "\n",
      "I am particularly enthusiastic about this role because it offers the opportunity to transition my strong\n",
      "technical foundation and passion for data science into a position focused on developing and\n",
      "optimizing AI solutions.\n",
      "I am eager to collaborate with your multi-functional team, leverage my\n",
      "problem-solving skills, and contribute to driving the success of your AI-assisted decision-making\n",
      "processes.\n",
      "\n",
      "Thank you for considering my application.\n",
      "I am looking forward to the opportunity to discuss how\n",
      "my background in software development, combined with my academic focus on data science, can\n",
      "contribute to the innovative projects.\n",
      "Please feel free to contact me at 8668566528 or via email at\n",
      "abhisw28@gmail.com to schedule an interview.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print (sent.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa880e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentece_tokenize = [sent.text for sent in doc.sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b7f7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am excited to apply for the Data Science Engineer position.',\n",
       " 'With a strong academic foundation in\\ndata science and analytics, complemented by practical experience as a software developer, I am\\neager to bring my skills to your team and contribute to the innovative AI-driven solutions your\\norganization is known for.\\n',\n",
       " 'My M.Tech in Data Science and Analytics has provided me with deep insights into machine\\nlearning, data analysis, and predictive modeling.',\n",
       " 'Although my professional experience at Vinz\\nGlobal and Aventior Digital Pvt Ltd primarily involved software development, these roles honed my\\ntechnical expertise, particularly in backend development, API improvement, and database\\nmigration.',\n",
       " 'These experiences have given me a solid understanding of the full software development\\nlifecycle, which is crucial for building and optimizing data pipelines, implementing machine\\nlearning models, and deploying data-driven applications.\\n',\n",
       " 'One of my significant academic projects involved developing a skin cancer detection and\\nclassification model using convolutional neural networks (CNNs).',\n",
       " 'This project not only enhanced\\nmy machine learning skills but also highlighted the importance of accuracy and innovation in\\ndeveloping life-impacting solutions.',\n",
       " 'Additionally, my work on a diamond price prediction model\\nusing linear regression demonstrated my ability to analyze complex datasets and create precise,\\ndata-driven forecasts.\\n',\n",
       " 'I am particularly enthusiastic about this role because it offers the opportunity to transition my strong\\ntechnical foundation and passion for data science into a position focused on developing and\\noptimizing AI solutions.',\n",
       " 'I am eager to collaborate with your multi-functional team, leverage my\\nproblem-solving skills, and contribute to driving the success of your AI-assisted decision-making\\nprocesses.\\n',\n",
       " 'Thank you for considering my application.',\n",
       " 'I am looking forward to the opportunity to discuss how\\nmy background in software development, combined with my academic focus on data science, can\\ncontribute to the innovative projects.',\n",
       " 'Please feel free to contact me at 8668566528 or via email at\\nabhisw28@gmail.com to schedule an interview.\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentece_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07e5a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize = [spacy_en.tokenizer(sent) for sent in sentece_tokenize]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f993ea9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mword_tokenize\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e45414dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = [[(token.text) for token in tokens] for tokens in word_tokenize]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4ba9e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "40\n",
      "['My', 'M.Tech', 'in', 'Data', 'Science', 'and', 'Analytics', 'has', 'provided', 'me', 'with', 'deep', 'insights', 'into', 'machine', '\\n', 'learning', ',', 'data', 'analysis', ',', 'and', 'predictive', 'modeling', '.']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_words[2]))\n",
    "print(len(tokenized_words[4]))\n",
    "print(tokenized_words[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea14cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_en(paragraph):\n",
    "    # Process the paragraph with spaCy\n",
    "    doc = spacy_en(paragraph)\n",
    "    corpus = []\n",
    "    \n",
    "    # Iterate through sentences\n",
    "    for sent in doc.sents:\n",
    "        # Clean and tokenize the sentence\n",
    "        words = [token.text.lower() for token in sent if token.is_alpha and not token.is_stop]\n",
    "        preprocessed_text = \" \".join(words)\n",
    "        corpus.append(preprocessed_text)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68c6e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = preprocessing_en(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "741bb16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_english(text):\n",
    "    sentences = [spacy_en.tokenizer(sent) for sent in text]\n",
    "    words = [[(token.text) for token in tokens] for tokens in sentences]\n",
    "    return words\n",
    "\n",
    "def tokenize_germany(text):\n",
    "    sentences = [sent.text for sent in text.sents]\n",
    "    sentences = [spacy_de.tokenizer(sent) for sent in sentences]\n",
    "    words = [[(token.text) for token in tokens] for tokens in sentences]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05d97a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['excited', 'apply', 'data', 'science', 'engineer', 'position'],\n",
       " ['strong',\n",
       "  'academic',\n",
       "  'foundation',\n",
       "  'data',\n",
       "  'science',\n",
       "  'analytics',\n",
       "  'complemented',\n",
       "  'practical',\n",
       "  'experience',\n",
       "  'software',\n",
       "  'developer',\n",
       "  'eager',\n",
       "  'bring',\n",
       "  'skills',\n",
       "  'team',\n",
       "  'contribute',\n",
       "  'innovative',\n",
       "  'ai',\n",
       "  'driven',\n",
       "  'solutions',\n",
       "  'organization',\n",
       "  'known'],\n",
       " ['data',\n",
       "  'science',\n",
       "  'analytics',\n",
       "  'provided',\n",
       "  'deep',\n",
       "  'insights',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'predictive',\n",
       "  'modeling'],\n",
       " ['professional',\n",
       "  'experience',\n",
       "  'vinz',\n",
       "  'global',\n",
       "  'aventior',\n",
       "  'digital',\n",
       "  'pvt',\n",
       "  'ltd',\n",
       "  'primarily',\n",
       "  'involved',\n",
       "  'software',\n",
       "  'development',\n",
       "  'roles',\n",
       "  'honed',\n",
       "  'technical',\n",
       "  'expertise',\n",
       "  'particularly',\n",
       "  'backend',\n",
       "  'development',\n",
       "  'api',\n",
       "  'improvement',\n",
       "  'database',\n",
       "  'migration'],\n",
       " ['experiences',\n",
       "  'given',\n",
       "  'solid',\n",
       "  'understanding',\n",
       "  'software',\n",
       "  'development',\n",
       "  'lifecycle',\n",
       "  'crucial',\n",
       "  'building',\n",
       "  'optimizing',\n",
       "  'data',\n",
       "  'pipelines',\n",
       "  'implementing',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'models',\n",
       "  'deploying',\n",
       "  'data',\n",
       "  'driven',\n",
       "  'applications'],\n",
       " ['significant',\n",
       "  'academic',\n",
       "  'projects',\n",
       "  'involved',\n",
       "  'developing',\n",
       "  'skin',\n",
       "  'cancer',\n",
       "  'detection',\n",
       "  'classification',\n",
       "  'model',\n",
       "  'convolutional',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'cnns'],\n",
       " ['project',\n",
       "  'enhanced',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'skills',\n",
       "  'highlighted',\n",
       "  'importance',\n",
       "  'accuracy',\n",
       "  'innovation',\n",
       "  'developing',\n",
       "  'life',\n",
       "  'impacting',\n",
       "  'solutions'],\n",
       " ['additionally',\n",
       "  'work',\n",
       "  'diamond',\n",
       "  'price',\n",
       "  'prediction',\n",
       "  'model',\n",
       "  'linear',\n",
       "  'regression',\n",
       "  'demonstrated',\n",
       "  'ability',\n",
       "  'analyze',\n",
       "  'complex',\n",
       "  'datasets',\n",
       "  'create',\n",
       "  'precise',\n",
       "  'data',\n",
       "  'driven',\n",
       "  'forecasts'],\n",
       " ['particularly',\n",
       "  'enthusiastic',\n",
       "  'role',\n",
       "  'offers',\n",
       "  'opportunity',\n",
       "  'transition',\n",
       "  'strong',\n",
       "  'technical',\n",
       "  'foundation',\n",
       "  'passion',\n",
       "  'data',\n",
       "  'science',\n",
       "  'position',\n",
       "  'focused',\n",
       "  'developing',\n",
       "  'optimizing',\n",
       "  'ai',\n",
       "  'solutions'],\n",
       " ['eager',\n",
       "  'collaborate',\n",
       "  'multi',\n",
       "  'functional',\n",
       "  'team',\n",
       "  'leverage',\n",
       "  'problem',\n",
       "  'solving',\n",
       "  'skills',\n",
       "  'contribute',\n",
       "  'driving',\n",
       "  'success',\n",
       "  'ai',\n",
       "  'assisted',\n",
       "  'decision',\n",
       "  'making',\n",
       "  'processes'],\n",
       " ['thank', 'considering', 'application'],\n",
       " ['looking',\n",
       "  'forward',\n",
       "  'opportunity',\n",
       "  'discuss',\n",
       "  'background',\n",
       "  'software',\n",
       "  'development',\n",
       "  'combined',\n",
       "  'academic',\n",
       "  'focus',\n",
       "  'data',\n",
       "  'science',\n",
       "  'contribute',\n",
       "  'innovative',\n",
       "  'projects'],\n",
       " ['feel', 'free', 'contact', 'email', 'schedule', 'interview']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_english(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56256a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.datasets import Multi30k\n",
    "# from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54aa1590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0\n",
      "0.14.0\n",
      "3.4.4\n",
      "1.23.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchtext.__version__)\n",
    "print(spacy.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a96a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_english = spacy.load('en_core_web_sm')\n",
    "spacy_german = spacy.load('de_core_news_sm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable_env_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
